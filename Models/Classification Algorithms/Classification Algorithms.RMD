---
title: 'Machine Learning'
date: "7/5/2022"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I will implement classical algorithms: KNN, Decision Tree, Random Forest. 

## Loading MNIST data set
I will train three algorithms on the MNIST training images and labels (N=60,000), and validate in order to tune my model using the MNIST testing images and labels (N=10,000). 

Individual MNIST digits are 28x28 grayscale images with pixel values between 0 and 255. A processed MNIST digit is "flattened" into an array of length 784 (28x28). The collection of MNIST digits are stored in a Matrix, with each row representing the array of one image. 500 handwritten digits should be processed into one Matrix with dimensions 784x500.

Labels are stored as an array of integers. 500 labels should be stored in one array, length 500.

```{Loading MNIST}
library(caret)
library(caTools)
library(class)
library(factoextra)
library(corrplot)
library(rpart)
library(rpart.plot)
library(randomForest)
library(imager)
library(FactoMineR)
library(forecast)
library(car)


show_digit = function(arr784, col = gray(12:1 / 12), ...) {image(matrix(as.matrix(arr784[-785]), nrow = 28)[, 28:1], col =gray(seq(0, 1, 0.05)), ...)}

load_image_file = function(filename) {
  ret = list()
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n    = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  nrow = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  ncol = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  x = readBin(f, 'integer', n = n * nrow * ncol, size = 1, signed = FALSE)
  close(f)
  data.frame(matrix(x, ncol = nrow * ncol, byrow = TRUE))
}


# load label files
load_label_file = function(filename) {
  f = file(filename, 'rb')
  readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  n = readBin(f, 'integer', n = 1, size = 4, endian = 'big')
  y = readBin(f, 'integer', n = n, size = 1, signed = FALSE)
  close(f)
  y
}

# load images
train = load_image_file("train-images-idx3-ubyte")
test  = load_image_file("t10k-images-idx3-ubyte")

# load labels
train$y = as.factor(as.character(load_label_file("train-labels-idx1-ubyte")))
test$y  = as.factor(load_label_file("t10k-labels-idx1-ubyte"))

set.seed(61710)

```


## Feature Engineering - Dimension Reduction using PCA.
### on MNIST data set
```{Feature Engineering}
#pca
trainPredictors.mnist = train[,-785]
testPredictors.mnist = test[,-785]

#determine number of components
#pca.mnist = PCA(trainPredictors.mnist, graph = F)
#pca.mnist$eig  
pca = prcomp(trainPredictors.mnist)
fviz_eig(pca,addlabels = T,n=260) #A 236-component structure suggested by the Scree plot. It would explain more than 90% of the original data. 

#apply component structure
train_components.mnist = data.frame(cbind(pca$x[,1:236], y=train['y']))
test_pca.mnist = predict(pca,newdata=testPredictors.mnist)
test_components.mnist = data.frame(cbind(test_pca.mnist[,1:236], y = test['y']))


#save to csv
write.csv(train_components.mnist,"C:/.../train_components.csv",row.names = FALSE)
write.csv(test_components.mnist,"C:/.../test_components.csv",row.names = FALSE)

#read
train_components.mnist = read.csv("C:/.../train_components.csv",header =TRUE)
test_components.mnist =  read.csv("C:/.../test_components.csv",header =TRUE)

```

### on 500 Handwritten Digits
```{Handwitten}
#read 500 handwritten digits
digits = as.matrix(read.table("C:/.../imageMatrix.txt", sep="\t",header=TRUE))
digits = as.data.frame(digits)

#apply pcs on new dataset
digits_pca = predict(pca,newdata=digits)
digits_components = data.frame(cbind(digits_pca[,1:236], y = digits$y))

digits_components[,'y'] = as.factor(digits_components[,'y'])

#save file
write.csv(digits_components,"C:/.../digits_components.csv",row.names = FALSE)

#read file
digits_components = read.csv("C:/.../digits_components.csv",header =TRUE)
```

## Data Modeling
### Classification Algorithms

Using the MNIST data set, you will train three algorithms on the MNIST training images and labels (N=60,000), and validate in order to tune your model using the MNIST testing images and labels (N=10,000). 

I will improve the algorithm by tuning parameters and re-validating and consider feature engineering, such as Dimension Reduction using PCA.

I will then test your algorithm using my 500 handwritten digits collected. 

Two elements demonstrating the result of my test:

1. The Confusion Matrix of the predicted vs true labels
2. The accuracy of predicting labels, as a percentage (e.g. 50%) 

#### 1) K-Nearest-Neighbors

Implement a K-Nearest-Neighbors classifier.
```{r KNN}
set.seed(61710)
model.knn <- knn(train = train_components.mnist, test = test_components.mnist,cl = train_components.mnist$y)

#Confusion Matrix and Accuracy with MNIST testing images and labels
table(`Actual Class` = test_components.mnist$y, `Predicted Class` = model.knn)
error.rate.knn <- sum(test_components.mnist$y != model.knn)/nrow(test_components.mnist)
accuracy_knn <- round((1 - error.rate.knn) *100,4)
accuracy_knn




##Tuning tree hyperparameters to address the threat of overfitting using 'cv' method
trControl <- trainControl(method = "cv", number = 5)
#tuneGrid = expand.grid(k = seq(2, 9, by = 1))
set.seed(61710)
cvModel.knn <- train(y~., data = train_components.mnist, 
                   method = "knn", 
                   trControl = trControl
                   #tuneGrid = tuneGrid
                   )
#cvModel.knn
#cvModel.knn$results
#plot(cvModel.knn)

cvKnn <- knn(train = train_components.mnist, test = test_components.mnist,cl = train_components.mnist$y,k=cvModel.knn$bestTune)

#Confusion Matrix and Accuracy with MNIST testing images and labels
table(`Actual Class` = test_components.mnist$y, `Predicted Class` = cvKnn)
error.rate.knn <- sum(test_components.mnist$y != cvKnn)/nrow(test_components.mnist)
accuracy_knn.t <- round((1 - error.rate.knn) *100,4)

#Confusion Matrix and Accuracy with digits dataset
cvKnn.d <- knn(train <- train_components.mnist, test = digits_components,cl = train_components.mnist$y,k=cvModel.knn$bestTune)
table(`Actual Class` <- digits_components$y, `Predicted Class` = cvKnn.d)
error.rate.knn.d <- sum(digits_components$y != cvKnn.d)/nrow(digits_components)
accuracy_knn.d <- round((1 - error.rate.knn.d) *100,4)

#accuracy of MNIST testing and Digits dataset
data.frame(Tree = c('Test', 'Digits'), 
           Accuracy = c(accuracy_knn.t, 
                        accuracy_knn.d)
           
) 
```

### 2B) Decision Tree

Implement a Decision Tree classifier.

```{r Q2B}
set.seed(61710)
model.tree <- rpart(y~.,method = "class", train_components.mnist)
pred.tree <- predict(model.tree, newdata = test_components.mnist, type = "class")

#Confusion Matrix and Accuracy with MNIST testing images and labels
cm.tree <- confusionMatrix(pred.tree, factor(test_components.mnist$y))
cm.tree$overall["Accuracy"]




#Tuning tree hyperparameters to address the threat of overfitting using 'cv' method
trControl.tree=trainControl(method="cv",number=5)
tuneGrid.tree = expand.grid(cp = seq(from = 0,to = 0.1,by = 0.001))
set.seed(61710)
cvModel.tree = train(y~.,data=train_components.mnist,
                     method="rpart",
                     trControl=trControl.tree,
                     tuneGrid=tuneGrid.tree 
                     )
cvModel.tree$results

cvTree = rpart(y~.,data=train_components.mnist,cp = cvModel.tree$bestTune$cp, method = "class")
plot(cvTree)

#Confusion Matrix and Accuracy with MNIST testing images and labels
pred.tree.t = predict(cvTree,newdata=test_components.mnist,type = "class")
cm.tree.t <- confusionMatrix(pred.tree.t, factor(test_components.mnist$y))

#Confusion Matrix and Accuracy with digits dataset
pred.tree.d <- predict(cvTree, digits_components, type="class")
cm.tree.d <- confusionMatrix(pred.tree.d, factor(digits_components$y))

#accuracy of MNIST testing and Digits dataset
data.frame(Tree = c('Test', 'Digits'), 
           Accuracy = c(cm.tree.t$overall["Accuracy"],
                        cm.tree.d$overall["Accuracy"])
           
) 

```
              
### 2C) Random Forest

Implement a Random Forest classifier according to the Q2 instructions above.

```{r Q2C}
#because of large data set and limited computational resources, subseting 10% of trainning data to do analysis
set.seed(61710)
split = sample(x=1:nrow(train_components.mnist),size=0.1*nrow(train_components.mnist))
train_subset =train_components.mnist[split,]

model.forest = randomForest(y~.,data=train_subset)
pred.forest = predict(model.forest,newdata=test_components.mnist)
#Confusion Matrix and Accuracy with MNIST testing images and labels
cm.forest <- confusionMatrix(pred.forest, factor(test_components.mnist$y))
cm.forest$overall["Accuracy"]

#Tuning tree hyperparameters to address the threat of overfitting using 'cv' method
trControl.forest=trainControl(method="cv",number=5)
grid = data.frame(mtry = c(1, 5, 10, 25, 50, 100))
set.seed(61710)
cvModel.forest = train(y~.,data=train_components.mnist,
                       method="rf",
                       ntree=1000,
                       trControl=trControl.forest,
                       tuneGrid=grid
)
cvModel.forest

cvForest = randomForest(y~.,data=train_subset,
                        #ntree = 10000,
                        mtry=25)
pred.forest.t = predict(cvForest,newdata=test_components.mnist)

#Confusion Matrix and Accuracy with MNIST testing images and labels
table(`Actual Class` = test_components.mnist$y, `Predicted Class` = cvForest)
error.rate.forest <- sum(test_components.mnist$y != cvKnn)/nrow(test_components.mnist)
accuracy_forest.t <- round((1 - error.rate.knn) *100,4)

#Confusion Matrix and Accuracy with digits dataset
pred.forest.d <- predict(cvForest, digits_components)
table(`Actual Class` = test_components.mnist$y, `Predicted Class` = cvForest)
error.rate.forest <- sum(test_components.mnist$y != cvForest)/nrow(test_components.mnist)
accuracy_forest.d <- round((1 - error.rate.knn) *100,4)

#accuracy of MNIST testing and Digits dataset
data.frame(Tree = c('Test', 'Digits'), 
           Accuracy = c(accuracy_forest.t,
                        accuracy_forest.d)
           
) 


```

### Retrospective

KNN model makes predictions by an iterative process finding nearest neighbors. 
The decision tree makes predictions based on a tree-shaped diagram that displays possible outputs. 
The random forest makes predictions by builds an ensemble of decision trees with randomly creation and merge. 
From the accuracy table, the KNN model has the best performance. It has achieved 97% accuracy and 27.8% accuracy. The decision tree has the worst performance.
